{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataset to train Llama 2 to play chess\n",
    "\n",
    "This notebook imports a dataset of chess moves from professional tournaments from a CSV. Each row contains a string which outlines every move in the tournament. Each move is numbered with an integer and period (e.g. Move 12 is written as \"12.\") with an uppercase letter for the piece, lower case letters for the rank, integers for the file and 'x' for a take.\n",
    "\n",
    "The legend for the chess pieces is listed below. The pawn has no key.\n",
    "* 'B': Bishop\n",
    "* 'K': King\n",
    "* 'N': Knight\n",
    "* 'P': Pawn\n",
    "* 'Q': Queen\n",
    "* 'R': Rook\n",
    "\n",
    "For example: \"Ng6\" denotes that a knight is moved to position g6, and \"Qxa4\" denotes that queen takes whatever is in a4.\n",
    "\n",
    "Each of the keys is replaced with a unique string which represents a unique token in the Llama 2 tokenizer. This is because for Llama to learn from these moves, we need to best translate it into the language that Llama understands: tokens. A set of 3 tokens is also added to the start of each row, so that this will signify to Llama that the rest of the tokens are chess moves.\n",
    "\n",
    "It should be noted that the term \"tokenize\" is being used to refer to the translation of the chess keys to the unique strings. However, when the model's tokenizer.tokenize method is called, what it does is convert the string into a list of token _IDs_ which are an integer which represents the unique token. Most libraries will train Llama 2 models using strings instead of the token IDs, which is why I convert the chess keys into the string tokens, rather than the token ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "moves = pd.read_csv('moves_raw.csv')[\"moves\"]\n",
    "\n",
    "eos = '</s>'\n",
    "black_move_token  = \"<0xFC>\"\n",
    "white_move_token  = \"<0xFD>\"\n",
    "chess_start_token = \"<0xF9><0xFF><0xFE>\"\n",
    "\n",
    "# # token to signify a take\n",
    "moves = moves.str.replace('x',\"<0xC9>\")\n",
    "\n",
    "# add chess_start_token to the front\n",
    "moves = moves.apply(lambda text: chess_start_token+text)\n",
    "\n",
    "# remove game results\n",
    "moves = moves.str.replace(\"1-0\",\"\")\n",
    "moves = moves.str.replace(\"0-0\",\"\")\n",
    "moves = moves.str.replace(\"1/2-1/2\",\"\")\n",
    "\n",
    "# remove move numbering and spaces\n",
    "moves = moves.apply(lambda x: re.sub(r'\\d+\\.', eos, x))\n",
    "moves = moves.str.replace(' '+eos, eos)\n",
    "moves = moves.str.replace(eos+' ', eos)\n",
    "moves = moves.str.replace(' ', eos)\n",
    "\n",
    "# replace positions with tokens\n",
    "moves = moves.str.replace('a',\"<0x0A>\")\n",
    "moves = moves.str.replace('b',\"<0x0C>\")\n",
    "moves = moves.str.replace('c',\"<0x0D>\")\n",
    "moves = moves.str.replace('d',\"<0x0E>\")\n",
    "moves = moves.str.replace('e',\"<0x0F>\")\n",
    "moves = moves.str.replace('f',\"<0x90>\")\n",
    "moves = moves.str.replace('g',\"<0x99>\")\n",
    "moves = moves.str.replace('h',\"<0x9A>\")\n",
    "moves = moves.str.replace('1',\"<0x9C>\")\n",
    "moves = moves.str.replace('2',\"<0x9D>\")\n",
    "moves = moves.str.replace('3',\"<0x9E>\")\n",
    "moves = moves.str.replace('4',\"<0x9F>\")\n",
    "moves = moves.str.replace('5',\"<0xA0>\")\n",
    "moves = moves.str.replace('6',\"<0xA9>\")\n",
    "moves = moves.str.replace('7',\"<0xAA>\")\n",
    "moves = moves.str.replace('8',\"<0xAC>\")\n",
    "\n",
    "# replace pieces with tokens\n",
    "moves = moves.str.replace('K',\"<0x10>\")\n",
    "moves = moves.str.replace('R',\"<0x11>\")\n",
    "moves = moves.str.replace('B',\"<0x12>\")\n",
    "moves = moves.str.replace('Q',\"<0x13>\")\n",
    "moves = moves.str.replace('N',\"<0x14>\")\n",
    "\n",
    "# append tokens to signify whose move it is\n",
    "# create this function to use in the apply method\n",
    "def turn_token(row):\n",
    "    row = row.split(eos) # split the text into a list divided by the eos token\n",
    "    text = ''\n",
    "    for i in range(len(row)):\n",
    "        if i%2: \n",
    "            # even moves are black\n",
    "            # insert the eos token and black_move_token\n",
    "            text += row[i]+eos+black_move_token \n",
    "        else: \n",
    "            # odd moves are white\n",
    "            # insert the eos token and white_move_token\n",
    "            text += row[i]+eos+white_move_token \n",
    "    \n",
    "    return text\n",
    "\n",
    "moves = moves.apply(turn_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Pandas Series as a csv\n",
    "\n",
    "moves.to_csv(\"tokenised_moves.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas Series into a JSON\n",
    "import json\n",
    "\n",
    "json_list = [{\"text\": text} for text in moves]\n",
    "with open('tokenised_moves.json', 'w') as f:\n",
    "    json.dump(json_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas Series into a parquet\n",
    "\n",
    "pd.DataFrame(moves).to_parquet(\"tokenised_moves.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-AI-to04sW5Y",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
